{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nCreated on Aug. 12, 2021\nGroup: Alpha IceCream\nTitle: A2C on LunarLander\n\"\"\"","metadata":{"id":"Dqjo3sULGr8Q","outputId":"3e41d6ff-1546-4609-f701-9c2475ecd948","execution":{"iopub.status.busy":"2021-08-13T12:20:32.916614Z","iopub.execute_input":"2021-08-13T12:20:32.91693Z","iopub.status.idle":"2021-08-13T12:20:32.93105Z","shell.execute_reply.started":"2021-08-13T12:20:32.91685Z","shell.execute_reply":"2021-08-13T12:20:32.929244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Install dependencies","metadata":{"id":"0RvoQaG8F-fM"}},{"cell_type":"code","source":"# # @title Install dependencies\n!pip install rarfile --quiet\n!pip install stable-baselines3 > /dev/null\n!pip install box2d-py > /dev/null\n!pip install gym pyvirtualdisplay > /dev/null 2>&1\n!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1","metadata":{"id":"emmjL3BfF5A3","execution":{"iopub.status.busy":"2021-08-13T12:20:32.935084Z","iopub.execute_input":"2021-08-13T12:20:32.935321Z","iopub.status.idle":"2021-08-13T12:21:02.488272Z","shell.execute_reply.started":"2021-08-13T12:20:32.935299Z","shell.execute_reply":"2021-08-13T12:21:02.487202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imports","metadata":{"id":"Ck23M3fFG7zH"}},{"cell_type":"code","source":"import io\nimport os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport base64\nimport stable_baselines3\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport gym\nfrom gym import spaces\nfrom gym.wrappers import Monitor","metadata":{"id":"_nlocVdkG6_3","execution":{"iopub.status.busy":"2021-08-13T12:53:31.350898Z","iopub.execute_input":"2021-08-13T12:53:31.351292Z","iopub.status.idle":"2021-08-13T12:53:31.357432Z","shell.execute_reply.started":"2021-08-13T12:53:31.351203Z","shell.execute_reply":"2021-08-13T12:53:31.356189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set device","metadata":{"id":"pstCUbN6ASf6"}},{"cell_type":"code","source":"def set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device\nset_device()","metadata":{"id":"wt0S1S8lARWz","execution":{"iopub.status.busy":"2021-08-13T12:53:31.359178Z","iopub.execute_input":"2021-08-13T12:53:31.35964Z","iopub.status.idle":"2021-08-13T12:53:31.371755Z","shell.execute_reply.started":"2021-08-13T12:53:31.359603Z","shell.execute_reply":"2021-08-13T12:53:31.370592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # @title Plotting/Video functions\n# from IPython.display import HTML\n# from pyvirtualdisplay import Display\n# from IPython import display as ipythondisplay\n\n# display = Display(visible=0, size=(1400, 900))\n# display.start()\n\n# \"\"\"\n# Utility functions to enable video recording of gym environment\n# and displaying it.\n# To enable video, just do \"env = wrap_env(env)\"\"\n# \"\"\"\n\n# def show_video():\n#   mp4list = glob.glob('video/*.mp4')\n#   if len(mp4list) > 0:\n#     mp4 = mp4list[0]\n#     video = io.open(mp4, 'r+b').read()\n#     encoded = base64.b64encode(video)\n#     ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n#                 loop controls style=\"height: 400px;\">\n#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n#              </video>'''.format(encoded.decode('ascii'))))\n#   else:\n#     print(\"Could not find video\")\n\n\n# def wrap_env(env):\n#   env = Monitor(env, './video', force=True)\n#   return env","metadata":{"id":"xSmCdAI880HA","execution":{"iopub.status.busy":"2021-08-13T12:53:31.374146Z","iopub.execute_input":"2021-08-13T12:53:31.374607Z","iopub.status.idle":"2021-08-13T12:53:31.381413Z","shell.execute_reply.started":"2021-08-13T12:53:31.37457Z","shell.execute_reply":"2021-08-13T12:53:31.380579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before training","metadata":{"id":"dxEwDwvmUuJn"}},{"cell_type":"code","source":"log_dir = \"/tmp/gym/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Create environment\nenv = gym.make(\"LunarLander-v2\")\n\n# On Kaggle, these two lines for visualization raise an error.\n#env = wrap_env(gym.make(\"LunarLander-v2\"))\n#env = stable_baselines3.common.monitor.Monitor(env, log_dir )\n\nn_episodes = 100\nfor episode in range(n_episodes):\n  observation = env.reset()\n  total_reward = 0\n  done = False\n  while not done:\n    # Visualize\n    #env.render()\n    action = env.action_space.sample() # 0- Do nothing, 1- Fire left engine, 2- Fire bottom engine, 3- Fire right engine\n    observation_, reward, done, info = env.step(action)\n    total_reward += reward\n    if done:\n      break;\n  if episode%10 == 0:\n    print('Episode: {}, Total Reward: {:.2f}.'.format(episode, total_reward))\n#env.close()\n#show_video()  ","metadata":{"id":"Se8u9V70G56D","outputId":"2d4ec0d9-1a51-4d2e-be46-5ad6de35ed55","execution":{"iopub.status.busy":"2021-08-13T12:53:31.383184Z","iopub.execute_input":"2021-08-13T12:53:31.38402Z","iopub.status.idle":"2021-08-13T12:53:32.81598Z","shell.execute_reply.started":"2021-08-13T12:53:31.383914Z","shell.execute_reply":"2021-08-13T12:53:32.815099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Policy network (Failed on RNN)","metadata":{"id":"O_m2z6-TUyYy"}},{"cell_type":"code","source":"# Loss\ncriterion = nn.CrossEntropyLoss()\n\nclass ActorCriticNetwork(nn.Module):\n  def __init__(self, lr, input_dim, hidden_dim, n_actions, gamma):\n\n    super(ActorCriticNetwork, self).__init__()\n    self.lr = lr\n    self.input_dim = input_dim\n    self.hidden_dim = hidden_dim\n    self.n_actions = n_actions\n    self.n_layers = n_layers\n    #self.model = model\n\n#     # Models\n#     if self.model == \"gru\":\n#       self.rnn = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, nonlinearity='relu')\n#     elif self.model == \"lstm\":\n#       self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, nonlinearity='relu')\n#     elif self.model == \"rnn\":\n#       self.rnn = nn.RNN(input_dim, hidden_dim, n_layers, batch_first=True, nonlinearity='relu')\n  \n    # Fully connected layer\n    self.layer1 = nn.Linear(*self.input_dim, self.hidden_dim)\n    self.layer2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n    self.layer_norm = nn.LayerNorm(self.hidden_dim)\n    self.pi = nn.Linear(self.hidden_dim, self.n_actions)\n    self.v = nn.Linear(self.hidden_dim, 1)\n    self.optimizer = torch.optim.Adam(self.parameters(), lr = lr)\n    \n    self.device = set_device()\n    self.to(self.device)\n\n  def forward(self, state):\n    # Initialize hidden state with zeros\n#     h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n    # One time step\n#     out, hn = self.rnn(state, h0)\n#     pi = self.pi(out[:, -1, :]) \n#     v = self.v(out[:, -1, :]) \n    x = F.relu(self.layer1(state))\n    x = self.layer_norm(x)\n    x = F.relu(self.layer2(x))\n    x = self.layer_norm(x)\n    pi = self.pi(x)\n    v = self.v(x)\n\n    return (pi, v)","metadata":{"id":"uUEW3M-y6dOL","execution":{"iopub.status.busy":"2021-08-13T12:55:55.508072Z","iopub.execute_input":"2021-08-13T12:55:55.508496Z","iopub.status.idle":"2021-08-13T12:55:55.519898Z","shell.execute_reply.started":"2021-08-13T12:55:55.50846Z","shell.execute_reply":"2021-08-13T12:55:55.518643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agent","metadata":{"id":"zqOE0PldVCuL"}},{"cell_type":"code","source":"class Agent():\n    def __init__(self, lr, input_dim, hidden_dim, n_actions, gamma):\n\n        self.lr = lr\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.n_actions = n_actions\n        self.gamma = gamma   # Discount factor\n#         self.n_layers = n_layers\n        #self.model = model   # RNN\n        self.network = ActorCriticNetwork(lr, input_dim, hidden_dim, n_actions, gamma)  \n        \n    def action_selection(self, observation):\n        state = torch.tensor([observation]).float().to(self.network.device)\n        action_prob, _ = self.network.forward(state)\n        action_prob = F.softmax(action_prob, dim=1)\n        action_dist = torch.distributions.Categorical(action_prob)\n        action = action_dist.sample()\n        action_log_prob = action_dist.log_prob(action)\n        self.action_log_prob = action_log_prob\n\n        return action.item()\n\n    def forward(self, state, reward, state_, done, advantage):\n        self.network.optimizer.zero_grad()\n\n        state = torch.tensor([state]).float().to(self.network.device)\n        state_ = torch.tensor([state_]).float().to(self.network.device)\n        reward = torch.tensor([reward]).float().to(self.network.device)\n        self.advantage = advantage\n\n        _, value = self.network.forward(state)\n        _, value_ = self.network.forward(state_)\n        \n        if self.advantage:\n            reward = reward + self.gamma * value_ *(1-int(done)) \n            \n            # https://gitee.com/nidao/Deep-reinforcement-learning-with-pytorch/blob/master/Char04%20A2C/A2C.py\n            advantage = reward - value\n            actor_loss  = -(self.action_log_prob * advantage.detach()).mean()\n            critic_loss = np.mean(self.advantage**2)\n            #loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n\n        else:\n            # In terminal(1-int(done)==1) state, the value will be 0. \n            # delta: the estimation error for updating the actor.\n            delta = reward + self.gamma * value_ *(1-int(done)) - value\n            actor_loss = -self.action_log_prob*delta\n            critic_loss = delta**2\n\n        (actor_loss + critic_loss).backward()\n        self.network.optimizer.step()","metadata":{"id":"ywpLLawODwXj","execution":{"iopub.status.busy":"2021-08-13T12:55:55.521705Z","iopub.execute_input":"2021-08-13T12:55:55.522346Z","iopub.status.idle":"2021-08-13T12:55:55.537244Z","shell.execute_reply.started":"2021-08-13T12:55:55.522309Z","shell.execute_reply":"2021-08-13T12:55:55.536439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting function","metadata":{"id":"U91cum4ZgeD5"}},{"cell_type":"code","source":"def plot_runing_curve(x, total_reward, figure_file):\n    running_avg = np.zeros(len(total_reward))\n    for i in range(len(running_avg)):\n        running_avg[i] = np.mean(total_reward[max(0, i-100):(i+1)])\n    plt.plot(x, running_avg)\n    plt.title('Running average of previous 100 total_reward')\n    plt.savefig(figure_file)","metadata":{"id":"5qhlihlYVntF","execution":{"iopub.status.busy":"2021-08-13T12:55:55.539031Z","iopub.execute_input":"2021-08-13T12:55:55.539528Z","iopub.status.idle":"2021-08-13T12:55:55.549911Z","shell.execute_reply.started":"2021-08-13T12:55:55.539487Z","shell.execute_reply":"2021-08-13T12:55:55.549068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameters","metadata":{"id":"IyBQ4xFs9Lvv"}},{"cell_type":"code","source":"#n_layers = 2\ninput_dim = [8]\nhidden_dim = 1024\nn_layers = 1\n#model = None\nlr = 0.0000001\ngamma = 0.999\nn_actions = 4\nadvantage = 1","metadata":{"id":"YHvAMTMx4f73","execution":{"iopub.status.busy":"2021-08-13T12:55:55.551686Z","iopub.execute_input":"2021-08-13T12:55:55.552193Z","iopub.status.idle":"2021-08-13T12:55:55.562909Z","shell.execute_reply.started":"2021-08-13T12:55:55.552155Z","shell.execute_reply":"2021-08-13T12:55:55.562138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n\n  env = gym.make(\"LunarLander-v2\")\n\n  n_episode = 1000\n  agent = Agent(lr, input_dim, hidden_dim, n_actions, gamma)\n\n  # Conventional terms used from https://gym.openai.com/docs/\n  total_reward = []\n  for episode in range(n_episode):\n      done = False\n      observation = env.reset()\n      temp_reward = 0\n      while not done:\n          action = agent.action_selection(observation)\n          state_, reward, done, info = env.step(action)\n          temp_reward += reward\n          agent.forward(observation, reward, observation_, done, advantage)\n          observation = observation_\n      total_reward.append(temp_reward)\n\n      reward_avg = np.mean(total_reward[-100:])\n      if episode %100 == 0:\n          print('episode ', episode, 'score %.1f' % temp_reward, 'average score %.1f' % reward_avg)\n\n  x = [episode+1 for episode in range(n_episode)]\n\n  fname = 'Actor_Critic_on_Lunar_Lander' + str(agent.lr) + '_' +str(n_episodes) + 'games'\n  plot_dir = \"/tmp/gym/plots\"\n  os.makedirs(plot_dir, exist_ok=True)\n  figure_file = os.path.join(plot_dir, fname +'.png')\n  plot_runing_curve(x, total_reward, figure_file)\n    \n  #   if done:\n  #     break;\n  env.close()\n  # show_video()  ","metadata":{"id":"NNXHq2eEhU8s","outputId":"2c857a6c-d3b8-4467-deb6-6e41102a6f4d","execution":{"iopub.status.busy":"2021-08-13T12:55:55.56415Z","iopub.execute_input":"2021-08-13T12:55:55.564543Z","iopub.status.idle":"2021-08-13T13:04:05.043587Z","shell.execute_reply.started":"2021-08-13T12:55:55.564508Z","shell.execute_reply":"2021-08-13T13:04:05.042788Z"},"trusted":true},"execution_count":null,"outputs":[]}]}